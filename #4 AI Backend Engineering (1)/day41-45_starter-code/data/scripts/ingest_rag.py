"""
ë£¨ë¯¸ ì„¸ê³„ê´€ ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì ì¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (ë©±ë“±ì„± ë³´ì¥)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. ê¸°ì¡´ documents í…Œì´ë¸” ë¹„ìš°ê¸° (truncate) â†’ ë©±ë“±ì„± ë³´ì¥
2. Markdown ë¬¸ì„œ ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
3. ë¬¸ì„œ ì²­í‚¹ (RecursiveCharacterTextSplitter)
4. Upstage Embeddingìœ¼ë¡œ ë²¡í„°í™”
5. Supabase pgvectorì— ì €ì¥

ì‹¤í–‰ ë°©ë²•:
    # ê¸°ë³¸: v2.5 (active) + v1.0 (deprecated) ëª¨ë‘ ì ì¬
    uv run python scripts/ingest_rag.py

    # v2.5 (active)ë§Œ ì ì¬ (Distractor ì œì™¸)
    uv run python scripts/ingest_rag.py --active-only

ë©±ë“±ì„±:
    ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ëª‡ ë²ˆì„ ì‹¤í–‰í•´ë„ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
    ì‹¤í–‰ ì‹œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚­ì œí•˜ê³  ìƒˆë¡œ ì ì¬í•©ë‹ˆë‹¤.

Distractorë€?
    RAG ì‹œìŠ¤í…œì˜ ë©”íƒ€ë°ì´í„° í•„í„°ë§ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•œ "ë°©í•´ ë¬¸ì„œ"ì…ë‹ˆë‹¤.
    - v2.5 (status="active"): ìµœì‹  ì •ë³´ (ì •ë‹µ)
    - v1.0 (status="deprecated"): ì´ì „ ì •ë³´ (Distractor)

    2ê°•ì—ì„œ í•„í„°ë§ ìœ ë¬´ì— ë”°ë¥¸ ê²€ìƒ‰ ê²°ê³¼ ì°¨ì´ë¥¼ ì‹œì—°í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ì˜ˆ: "ë£¨ë¯¸ê°€ ì¢‹ì•„í•˜ëŠ” ìŒì‹?"
        - í•„í„°ë§ ì—†ìŒ â†’ v1.0 "ë”¸ê¸°" (âŒ ì˜¤ë˜ëœ ì •ë³´)
        - status="active" í•„í„°ë§ â†’ v2.5 "ë”¸ê¸° ì•ŒëŸ¬ì§€" (âœ… ìµœì‹  ì •ë³´)

ì‹¤í–‰ ì „ í•„ìš”ì‚¬í•­:
    1. .env íŒŒì¼ì— UPSTAGE_API_KEY, SUPABASE_URL, SUPABASE_KEY ì„¤ì •
    2. Supabase SQL Editorì—ì„œ data/schema.sql ì‹¤í–‰
"""

import argparse
import asyncio
import json
import re
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from loguru import logger


def extract_metadata(content: str) -> dict:
    """
    ë¬¸ì„œ ìƒë‹¨ì˜ RAG_METADATA ë¸”ë¡ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

    ë¬¸ì„œ í˜•ì‹ ì˜ˆì‹œ:
        <!--
        RAG_METADATA:
        {
          "version": "2.5",
          "status": "active"
        }
        -->

    Args:
        content: ë¬¸ì„œ ë‚´ìš©

    Returns:
        dict: ì¶”ì¶œëœ ë©”íƒ€ë°ì´í„° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
    """
    pattern = r"RAG_METADATA:\s*(\{[\s\S]*?\})\s*-->"
    match = re.search(pattern, content)

    if match:
        try:
            metadata = json.loads(match.group(1))
            logger.info(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì„±ê³µ: {metadata.get('version', 'unknown')}")
            return metadata
        except json.JSONDecodeError as e:
            logger.warning(f"ë©”íƒ€ë°ì´í„° JSON íŒŒì‹± ì‹¤íŒ¨: {e}")

    # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
    return {
        "version": "unknown",
        "status": "active",
        "document_type": "character_profile",
    }


def chunk_document(
    content: str, chunk_size: int = 500, chunk_overlap: int = 50
) -> list[str]:
    """
    ë¬¸ì„œë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

    Markdown ë¬¸ì„œì˜ ê²½ìš° ì„¹ì…˜ êµ¬ë¶„ìë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

    Args:
        content: ë¬¸ì„œ ë‚´ìš©
        chunk_size: ì²­í¬ ìµœëŒ€ í¬ê¸° (ë¬¸ì ìˆ˜)
        chunk_overlap: ì²­í¬ ê°„ ì¤‘ë³µ í¬ê¸°

    Returns:
        list[str]: ì²­í¬ ëª©ë¡
    """
    from langchain_text_splitters import RecursiveCharacterTextSplitter

    # ë§ˆí¬ë‹¤ìš´ ì„¹ì…˜ êµ¬ë¶„ì ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=[
            "\n## ",  # H2 í—¤ë”
            "\n### ",  # H3 í—¤ë”
            "\n#### ",  # H4 í—¤ë”
            "\n\n",  # ë¹ˆ ì¤„
            "\n",  # ì¤„ë°”ê¿ˆ
            " ",  # ê³µë°±
        ],
        length_function=len,
    )

    chunks = splitter.split_text(content)
    logger.info(f"ë¬¸ì„œë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")

    return chunks


async def embed_chunks(chunks: list[str]) -> list[list[float]]:
    """
    ì²­í¬ë“¤ì„ Upstage Embeddingìœ¼ë¡œ ë²¡í„°í™”í•©ë‹ˆë‹¤.

    Args:
        chunks: ì²­í¬ ëª©ë¡

    Returns:
        list[list[float]]: ì„ë² ë”© ë²¡í„° ëª©ë¡
    """
    from langchain_upstage import UpstageEmbeddings
    from app.core.config import settings

    if not settings.upstage_api_key:
        raise ValueError("UPSTAGE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    embeddings = UpstageEmbeddings(
        api_key=settings.upstage_api_key,
        model="solar-embedding-1-large-passage",  # 4096ì°¨ì›
    )

    logger.info(f"{len(chunks)}ê°œ ì²­í¬ ì„ë² ë”© ì‹œì‘...")

    # ë°°ì¹˜ë¡œ ì„ë² ë”© (API í˜¸ì¶œ ìµœì†Œí™”)
    vectors = await embeddings.aembed_documents(chunks)

    logger.info(f"ì„ë² ë”© ì™„ë£Œ: {len(vectors)}ê°œ ë²¡í„° (ì°¨ì›: {len(vectors[0])})")

    return vectors


async def truncate_documents() -> int:
    """
    documents í…Œì´ë¸”ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤. (ë©±ë“±ì„± ë³´ì¥)

    Returns:
        int: ì‚­ì œëœ ë ˆì½”ë“œ ìˆ˜
    """
    from supabase import create_client
    from app.core.config import settings

    if not settings.supabase_url or not settings.supabase_key:
        raise ValueError("Supabase ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    client = create_client(settings.supabase_url, settings.supabase_key)

    # ê¸°ì¡´ ë°ì´í„° ìˆ˜ í™•ì¸
    existing = client.table("documents").select("id", count="exact").execute()
    existing_count = existing.count or 0

    if existing_count > 0:
        # ëª¨ë“  ë°ì´í„° ì‚­ì œ (UUID íƒ€ì…ì´ë¯€ë¡œ content í•„ë“œë¡œ ë¹„êµ)
        client.table("documents").delete().neq("content", "").execute()
        logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° {existing_count}ê°œ ì‚­ì œ ì™„ë£Œ")
    else:
        logger.info("ğŸ“­ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ")

    return existing_count


async def save_to_supabase(
    chunks: list[str], vectors: list[list[float]], metadata: dict
) -> int:
    """
    ì²­í¬ì™€ ë²¡í„°ë¥¼ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        chunks: ì²­í¬ ëª©ë¡
        vectors: ì„ë² ë”© ë²¡í„° ëª©ë¡
        metadata: ë¬¸ì„œ ë©”íƒ€ë°ì´í„°

    Returns:
        int: ì €ì¥ëœ ë ˆì½”ë“œ ìˆ˜
    """
    from supabase import create_client
    from app.core.config import settings

    if not settings.supabase_url or not settings.supabase_key:
        raise ValueError("Supabase ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    client = create_client(settings.supabase_url, settings.supabase_key)

    logger.info("Supabaseì— ì €ì¥ ì‹œì‘...")

    saved_count = 0
    for i, (chunk, vector) in enumerate(zip(chunks, vectors)):
        try:
            # ê° ì²­í¬ë³„ ë©”íƒ€ë°ì´í„° (ì›ë³¸ + ì²­í¬ ì¸ë±ìŠ¤)
            chunk_metadata = {**metadata, "chunk_index": i, "chunk_total": len(chunks)}

            result = (
                client.table("documents")
                .insert(
                    {"content": chunk, "embedding": vector, "metadata": chunk_metadata}
                )
                .execute()
            )

            saved_count += 1

            if (i + 1) % 10 == 0:
                logger.info(f"ì§„í–‰ ì¤‘: {i + 1}/{len(chunks)}")

        except Exception as e:
            logger.error(f"ì²­í¬ {i} ì €ì¥ ì‹¤íŒ¨: {e}")

    logger.info(f"Supabase ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")

    return saved_count


async def ingest_document(file_path: str) -> dict:
    """
    ë‹¨ì¼ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        file_path: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ

    Returns:
        dict: ì²˜ë¦¬ ê²°ê³¼ (chunks, saved, metadata)
    """
    path = Path(file_path)

    if not path.exists():
        raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

    logger.info(f"ë¬¸ì„œ ë¡œë“œ: {path.name}")

    # 1. ë¬¸ì„œ ë¡œë“œ
    content = path.read_text(encoding="utf-8")

    # 2. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    metadata = extract_metadata(content)

    # 3. ì²­í‚¹
    chunks = chunk_document(content)

    # 4. ì„ë² ë”©
    vectors = await embed_chunks(chunks)

    # 5. ì €ì¥
    saved_count = await save_to_supabase(chunks, vectors, metadata)

    return {
        "file": path.name,
        "chunks": len(chunks),
        "saved": saved_count,
        "metadata": metadata,
    }


def parse_args():
    """ëª…ë ¹ì¤„ ì¸ìë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
    parser = argparse.ArgumentParser(
        description="ë£¨ë¯¸ ì„¸ê³„ê´€ ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì ì¬í•©ë‹ˆë‹¤. (ë©±ë“±ì„± ë³´ì¥)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê¸°ë³¸: v2.5 (active) + v1.0 (deprecated) ëª¨ë‘ ì ì¬
  uv run python data/scripts/ingest_rag.py

  # v2.5 (active)ë§Œ ì ì¬ (Distractor ì œì™¸)
  uv run python data/scripts/ingest_rag.py --active-only

ë©±ë“±ì„±:
  ëª‡ ë²ˆì„ ì‹¤í–‰í•´ë„ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë³´ì¥í•©ë‹ˆë‹¤.
  ì‹¤í–‰ ì‹œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚­ì œí•˜ê³  ìƒˆë¡œ ì ì¬í•©ë‹ˆë‹¤.

Distractor ì„¤ëª…:
  v1.0 (deprecated)ì€ RAG ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì‹œì—°ìš© "ë°©í•´ ë¬¸ì„œ"ì…ë‹ˆë‹¤.
  2ê°•ì—ì„œ í•„í„°ë§ ìœ ë¬´ì— ë”°ë¥¸ ê²€ìƒ‰ ê²°ê³¼ ì°¨ì´ë¥¼ ì‹œì—°í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """,
    )
    parser.add_argument(
        "--active-only",
        action="store_true",
        help="v2.5 (active) ë¬¸ì„œë§Œ ì ì¬ (Distractor ì œì™¸)",
    )
    return parser.parse_args()


async def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ë©±ë“±ì„± ë³´ì¥)

    ê¸°ë³¸: v2.5 (active) + v1.0 (deprecated) ëª¨ë‘ ì ì¬
    --active-only: v2.5ë§Œ ì ì¬ (Distractor ì œì™¸)
    """
    args = parse_args()

    logger.info("=" * 60)
    logger.info("RAG ë°ì´í„° Ingestion (ë©±ë“±ì„± ë³´ì¥)")
    logger.info("=" * 60)

    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ data í´ë” (í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜: data/scripts/ingest_rag.py)
    # parent=scripts, parent.parent=data
    data_dir = Path(__file__).parent.parent

    # ì ì¬í•  íŒŒì¼ ëª©ë¡ (ê¸°ë³¸: ë‘˜ ë‹¤ ì ì¬)
    files_to_ingest = [
        ("lumi_worldview_v2.5.md", "active", "ìµœì‹  ì •ë³´ (ì •ë‹µ)"),
    ]

    if not args.active_only:
        files_to_ingest.append(
            ("lumi_worldview_v1.0.md", "deprecated", "ì´ì „ ì •ë³´ (Distractor)")
        )
    else:
        logger.info("ğŸ“Œ --active-only ëª¨ë“œ: v2.5 (active)ë§Œ ì ì¬í•©ë‹ˆë‹¤.")
    logger.info("")

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    for filename, _, _ in files_to_ingest:
        file_path = data_dir / filename
        if not file_path.exists():
            logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            logger.info(f"ë¨¼ì € data/{filename} íŒŒì¼ì„ ìƒì„±í•˜ì„¸ìš”.")
            return

    try:
        # 1. ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ë©±ë“±ì„± ë³´ì¥)
        logger.info("ğŸ”„ Step 1: ê¸°ì¡´ ë°ì´í„° ì •ë¦¬")
        await truncate_documents()

        # 2. ë¬¸ì„œ ì ì¬
        logger.info("\nğŸ”„ Step 2: ë¬¸ì„œ ì ì¬")
        results = []
        total_chunks = 0
        total_saved = 0

        for filename, expected_status, description in files_to_ingest:
            file_path = data_dir / filename
            logger.info(f"\nğŸ“„ {filename} ({description})")
            logger.info("-" * 40)

            result = await ingest_document(str(file_path))
            results.append(result)
            total_chunks += result["chunks"]
            total_saved += result["saved"]

            # ë©”íƒ€ë°ì´í„° ê²€ì¦
            actual_status = result["metadata"].get("status", "unknown")
            if actual_status != expected_status:
                logger.warning(
                    f"âš ï¸ ë©”íƒ€ë°ì´í„° ë¶ˆì¼ì¹˜: ì˜ˆìƒ={expected_status}, ì‹¤ì œ={actual_status}"
                )

        # ìµœì¢… ê²°ê³¼
        logger.info("\n" + "=" * 60)
        logger.info("Ingestion ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 60)
        for result in results:
            status_emoji = (
                "âœ…" if result["metadata"].get("status") == "active" else "ğŸ“¦"
            )
            logger.info(
                f"{status_emoji} {result['file']}: {result['saved']}/{result['chunks']} ì²­í¬"
            )
        logger.info("-" * 40)
        logger.info(f"ì´ ì²­í¬: {total_chunks}ê°œ, ì €ì¥: {total_saved}ê°œ")
        logger.info("=" * 60)

        logger.success("RAG Ingestion ì™„ë£Œ!")

    except ValueError as e:
        logger.error(f"ì„¤ì • ì˜¤ë¥˜: {e}")
        logger.info("1. .env íŒŒì¼ì— UPSTAGE_API_KEY ì„¤ì •")
        logger.info("2. .env íŒŒì¼ì— SUPABASE_URL, SUPABASE_KEY ì„¤ì •")
        logger.info("3. Supabase SQL Editorì—ì„œ data/schema.sql ì‹¤í–‰")

    except Exception as e:
        logger.error(f"Ingestion ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
